{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb69130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f851adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = r'D:\\UniWien-Sem3\\BI2\\CAISE2025_Paper\\GraphGPS\\PGTNet4SSD\\raw_dataset'\n",
    "dict_folder = r'D:\\UniWien-Sem3\\BI2\\CAISE2025_Paper\\GraphGPS\\PGTNet4SSD\\raw_dataset'\n",
    "dummy_folder = r'D:\\UniWien-Sem3\\BI2\\CAISE2025_Paper\\GraphGPS\\PGTNet4SSD\\baselines\\dummy'\n",
    "case_column = 'concept:name'\n",
    "timestamp_column = 'time:timestamp'\n",
    "split_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "ssd_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2f8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_test_dataframes(dataset_name):\n",
    "    log_path = os.path.join(log_folder, dataset_name + '.xes')\n",
    "    dict_path = os.path.join(dict_folder, dataset_name + '.pkl')\n",
    "    df_path = os.path.join(dummy_folder, dataset_name + '.csv')\n",
    "    df_ssd_path = os.path.join(dummy_folder, dataset_name + '_ssd_.csv')\n",
    "    log = xes_importer.apply(log_path) \n",
    "    with open(dict_path, 'rb') as f:\n",
    "        ssd_dict =  pickle.load(f)\n",
    "    selected_cases = ssd_dict.get(ssd_id) \n",
    "    subset_log = pm4py.filter_trace_attribute_values(log, 'concept:name', selected_cases)\n",
    "    prefix_dict = {'case_id': [], 'prefix_length': [], 'start': [], 'rem_time': [], \n",
    "                   'train': [], 'val': [], 'test': []}\n",
    "    # iterate over all cases of event log\n",
    "    for i in range (len(log)): \n",
    "        current_case = log[i]\n",
    "        case_id = current_case.attributes.get(case_column)\n",
    "        case_length = len(current_case)\n",
    "        start_date = current_case[0].get(timestamp_column)\n",
    "        end_date = current_case[case_length-1].get(timestamp_column)\n",
    "        # iterate over all prefixes of each case\n",
    "        # j represent the prefix length\n",
    "        for j in range(2, case_length):\n",
    "            event_time = current_case[j-1].get(timestamp_column)\n",
    "            rem_time = (end_date - event_time).total_seconds()/3600/24\n",
    "            prefix_dict['case_id'].append(case_id)\n",
    "            prefix_dict['prefix_length'].append(j)\n",
    "            prefix_dict['start'].append(start_date)\n",
    "            prefix_dict['rem_time'].append(rem_time)\n",
    "            prefix_dict['train'].append(0)\n",
    "            prefix_dict['val'].append(0)\n",
    "            prefix_dict['test'].append(0)\n",
    "            \n",
    "    prefix_dict2 = {'case_id': [], 'prefix_length': [], 'start': [], 'rem_time': [], \n",
    "                   'train': [], 'val': [], 'test': []}\n",
    "    # iterate over all cases of event log\n",
    "    for i in range (len(subset_log)): \n",
    "        current_case = subset_log[i]\n",
    "        case_id = current_case.attributes.get(case_column)\n",
    "        case_length = len(current_case)\n",
    "        start_date = current_case[0].get(timestamp_column)\n",
    "        end_date = current_case[case_length-1].get(timestamp_column)\n",
    "        # iterate over all prefixes of each case\n",
    "        # j represent the prefix length\n",
    "        for j in range(2, case_length):\n",
    "            event_time = current_case[j-1].get(timestamp_column)\n",
    "            rem_time = (end_date - event_time).total_seconds()/3600/24\n",
    "            prefix_dict2['case_id'].append(case_id)\n",
    "            prefix_dict2['prefix_length'].append(j)\n",
    "            prefix_dict2['start'].append(start_date)\n",
    "            prefix_dict2['rem_time'].append(rem_time)\n",
    "            prefix_dict2['train'].append(0)\n",
    "            prefix_dict2['val'].append(0)\n",
    "            prefix_dict2['test'].append(0)\n",
    "            \n",
    "    index_df = pd.DataFrame(prefix_dict) \n",
    "    ssd_df = pd.DataFrame(prefix_dict2)\n",
    "    df_sorted = index_df.sort_values(by='start')\n",
    "    ssd_df_sorted = ssd_df.sort_values(by='start')\n",
    "    unique_case_ids = df_sorted['case_id'].drop_duplicates().tolist()\n",
    "    unique_case_ids2 = ssd_df_sorted['case_id'].drop_duplicates().tolist()\n",
    "    train_val_idx = int(len(unique_case_ids) * split_ratio)\n",
    "    train_idx = int(train_val_idx * (1-val_ratio))\n",
    "    train_case_ids = unique_case_ids[:train_idx]\n",
    "    val_case_ids = unique_case_ids[train_idx:train_val_idx]\n",
    "    test_case_ids = unique_case_ids[train_val_idx:]\n",
    "    index_df['train'] = index_df['case_id'].apply(lambda x: 1 if x in train_case_ids else 0)\n",
    "    index_df['val'] = index_df['case_id'].apply(lambda x: 1 if x in val_case_ids else 0)\n",
    "    index_df['test'] = index_df['case_id'].apply(lambda x: 1 if x in test_case_ids else 0)\n",
    "    train_val_idx = int(len(unique_case_ids2) * split_ratio)\n",
    "    train_idx = int(train_val_idx * (1-val_ratio))\n",
    "    train_case_ids = unique_case_ids2[:train_idx]\n",
    "    val_case_ids = unique_case_ids2[train_idx:train_val_idx]\n",
    "    test_case_ids = unique_case_ids2[train_val_idx:]\n",
    "    ssd_df['train'] = ssd_df['case_id'].apply(lambda x: 1 if x in train_case_ids else 0)\n",
    "    ssd_df['val'] = ssd_df['case_id'].apply(lambda x: 1 if x in val_case_ids else 0)\n",
    "    ssd_df['test'] = ssd_df['case_id'].apply(lambda x: 1 if x in test_case_ids else 0)\n",
    "    index_df.to_csv(df_path, index=False)\n",
    "    ssd_df.to_csv(df_ssd_path, index=False)\n",
    "    train_val_df = index_df[(index_df['train'] == 1) | (index_df['val'] == 1)]\n",
    "    test_df = index_df[index_df['test'] == 1]\n",
    "    unique_cases1 = len(train_val_df['case_id'].unique())\n",
    "    unique_cases2 = len(test_df['case_id'].unique())\n",
    "    print(unique_cases1, unique_cases2)\n",
    "    train_val_ssd_df = ssd_df[(ssd_df['train'] == 1) | (ssd_df['val'] == 1)]\n",
    "    test_ssd_df = ssd_df[ssd_df['test'] == 1]\n",
    "    return train_val_df, test_df, train_val_ssd_df, test_ssd_df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1252d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_average(key, dictionary):\n",
    "    if key in dictionary:\n",
    "        return dictionary[key]\n",
    "    else:\n",
    "        print('condition is met')\n",
    "        # Get the closest keys\n",
    "        keys = np.array(list(dictionary.keys()))\n",
    "        closest_keys = keys[np.argsort(np.abs(keys - key))[:2]]  # Get two closest keys\n",
    "        return dictionary[closest_keys[0]] if len(closest_keys) == 1 else np.mean([dictionary[k] for k in closest_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e44eac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 100000/100000 [00:27<00:00, 3626.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55401 13851\n",
      "47.86833203947014 66.83631973557166\n",
      "MAE:  47.86833203947014\n",
      "MAE_SSD:  66.83631973557166\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset_name = 'Hospital' # BPIC20_DomesticDeclarations BPIC20_InternationalDeclarations BPIC20RFP BPIC20PTC BPIC20TPD BPIC15_1 BPIC13I BPIC12 HelpDesk Sepsis\n",
    "\n",
    "train_val_df, test_df, train_val_ssd_df, test_ssd_df  = get_training_test_dataframes(dataset_name)\n",
    "prefix_lengths = train_val_df['prefix_length'].drop_duplicates().tolist()\n",
    "prefix_ssd_lengths = train_val_ssd_df['prefix_length'].drop_duplicates().tolist()\n",
    "dummy_dict, dummy_ssd_dict = {}, {}\n",
    "for length in prefix_lengths:\n",
    "    selected_rows = train_val_df[train_val_df['prefix_length'] == length]\n",
    "    dummy_dict[length] = selected_rows['rem_time'].mean()\n",
    "for length in prefix_ssd_lengths:\n",
    "    selected_rows = train_val_ssd_df[train_val_ssd_df['prefix_length'] == length]\n",
    "    dummy_ssd_dict[length] = selected_rows['rem_time'].mean()   \n",
    "\n",
    "test_df['dummy_predictions'] = test_df['prefix_length'].apply(lambda x: get_closest_average(x, dummy_dict))\n",
    "test_ssd_df['dummy_predictions'] = test_ssd_df['prefix_length'].apply(lambda x: get_closest_average(x, dummy_ssd_dict))\n",
    "MAE = (test_df['rem_time'] - test_df['dummy_predictions']).abs().mean()\n",
    "MAE_SSD = (test_ssd_df['rem_time'] - test_ssd_df['dummy_predictions']).abs().mean()\n",
    "print(MAE, MAE_SSD)\n",
    "print(\"MAE: \", MAE)\n",
    "print(\"MAE_SSD: \", MAE_SSD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
